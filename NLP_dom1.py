from nltk import word_tokenize
print(word_tokenize("This is an example sentence used for word segmentation."))